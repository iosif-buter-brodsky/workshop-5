# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #1 выполнил(а):
- Цюприк Егор Васильевич
- РИ220936
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Ознакомиться с основными рабыты с ML агентами на Unity.

## Задание 1
###  Найдите внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели.
#### коэффициент корреляции:
```py
if(distanceToTarget < 1.42f)
{
    SetReward(1.0f);
    EndEpisode();
}

```
### Данный параметр определяет на каком расстоянии от цели агент будет успешен. То есть при увеличении коэффицента - модель будет обучаться быстрее, но менее точно и наоборот при уменьшении коэффицента, модель будет обучаться медленее, но точнее.

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.
#### параметры файла yaml-агента:
```py
behaviors:
  RollerBall:
    trainer_type: ppo
    hyperparameters:
      batch_size: 10
      buffer_size: 100
      learning_rate: 3.0e-4
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
```
### max_steps - количество шагов необходимое модели для полного обучения. Чем их больше, тем лучше результат и тем долше время обучения и тем больше нужно вычислительных ресурсов.
### normalize - флаг, определяющий, нужно ли нормализовать входные данные перед подачей их на вход нейронной сети. Помогает ускорить обучение и стабилизировать процесс.
### num_epoch - количество эпох обучения, то есть сколько раз модель проходит через все данные. Увеличение параметра может улучшить качество обучения за счет более глубокого обновления модели, но учеличит время.



## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 
#### 1) npc напарник, который должен постоянно следовать за игроком. 
#### 2) алгоритм выдачи наград игроку основанный на большом количестве экономических параметров 
#### ml агент может быть более эффективным и удобным в случаях, когда задача требует анализа больших объемов данных, сложных взаимодействий и имеет много нелинейных зависимостей. 

## Выводы

Абзац умных слов о том, что было сделано и что было узнано.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
